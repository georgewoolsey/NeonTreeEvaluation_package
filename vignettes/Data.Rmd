---
title: "Working with data in the NeonTreeEvaluation Benchmark"
author: "Ben Weinstein"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The goal of this vignette is to provide a demonstration of how to work with the annotated data and visualize each sensor type. For an example of evaluating a novel submission to the benchmark see the Evaluation vignette. 

```{r setup, include = FALSE}
TRAVIS <- !identical(tolower(Sys.getenv("TRAVIS")), "true")
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  purl = TRAVIS
)
```

How to read and view xml annotations from the NEONTreeEvaluation benchmark dataset

```{r, include=FALSE}
library(knitr)
opts_chunk$set(warning = FALSE,message = FALSE)
```

```{r}
library(raster)
library(dplyr)
library(lidR)
library(NeonTreeEvaluation)

#Check if the data has been downloaded
download()
```

## Find data within the package

The get_data function allows users to find the sensor or annotation data for a particular plot in the dataset without having to know where the package is installed on a users computer.

For example, if I wanted the path to the RGB and LiDAR data for plot SJER_052.

```{r}
rgb_path<-get_data("SJER_052_2018","rgb")
rgb_path
lidar_path<-get_data("SJER_052_2018","lidar")
```
## Which data go with which evaluation metrics?

This package provides access to image-annotated crowns, field-annotated crowns and field surveyed tree stems as evaluation data. Not all images have all types of evaluation data. In general, it is recommended to just predict all data in the rgb images and let the functions decide which data to use.

```{r}
rgb_images<-list_rgb()
head(rgb_images)
```
However, for greater control there are helper functions to see which data fits for which evaluation routine. For example for field crowns:

```{r}
head(list_field_crowns())
```

## Annotations

For each training and evaluation site, we manually hand-annotated images using the program RectLabel. For each visible tree, we created a bounding box (xmin, ymin, xmax, ymax) that covered the tree crown. The coordinates of this box are relative to the top left corner of each image. 

Annotations are stored as xml json files

```{r}
xml<-get_data("SJER_052_2018","annotations")
xml
```
and can be read using the xml_parse function

```{r}
xml_parse(xml)
```

## Orthorectified Camera Mosaic (‘RGB’ NEON ID: DP3.30010.001)

The RGB data consist of images taken with a D8900 camera with a format of 8,984 x 6,732 pixels. Individual images are color rectified, orthorectified and mosaiced to create a single raster image with a pixel size of 0.1 m^2. Mosaic tiles are provided as 1km^2 geoTIFF files and are named based on the utm coordinate at the northwest origin. RGB data have high spatial resolution and individual trees are often visible based on the crown boundary, as well as color differences among individuals due to taxonomy and health status. 

```{r}
rgb<-get_data("SJER_052_2018","rgb")
img<-stack(rgb)
```

To compare the image to the annotations, it is easier to overlay the bounding boxes in geographic coordinates. This package has a projection utility (\code{boxes_to_spatial_polygons}) using the RGB image.

```{r}
#View one plot's annotations as polygons, project into UTM
#copy project utm zone (epsg), xml has no native projection metadata
annotations<-xml_parse(xml)
ground_truth <- boxes_to_spatial_polygons(annotations,img)

plotRGB(img)
plot(ground_truth,add=T)
```

## Classified LiDAR Point Cloud (NEON ID: DP1.30003.001)
The LiDAR data are 3D coordinates (4-6 points/m2) that provide high resolution information about tree crown shape and height. LiDAR data is stored as 1km^2 .laz files (Figure 2). These files contain the x,y,z coordinates for each return, as well as metadata on return intensity and point classification. The LiDAR data has been normalized with respect to classified ground points to standardize height measurements. Tree crowns are often apparent due to gaps among neighboring trees or differences in height among overlapping crowns. 


Now let's load the corresponding LiDAR point cloud for this evaluation plot.

```{r}
point_cloud_path<-get_data("SJER_052_2018","lidar")
point_cloud<-readLAS(point_cloud_path)
```
Tree annotations are stored in the UserData column. Each tree has a unique numeric ID

```{r}
plot(point_cloud)
plot(point_cloud,color="label")
tree_only<-lasfilter(point_cloud,!point_cloud@data$label==0)
plot(tree_only,color="label")
```

## Hyperspectral surface reflectance (NEON ID: DP1.30006.001)
NEON’s pushbroom style instrument collects visible and infrared spectrum from between approximately 420-2500 nm with a spectral sampling interval of 5nm for a total of 426 bands. NEON provides the orthorectified images with a pixel size of 1 m2 in 1 km2 tiles that align with the RGB and LiDAR file naming convention. Hyperspectral data, especially in the infrared spectrum, is often used for differentiating tree species (e.g. Maschler et al. 2018). In forests with high species diversity, these data may be used to delineate crown boundaries among neighboring trees.

```{r}
path<-get_data("MLBS_071_2018",type="hyperspectral")
g<-stack(path)
nlayers(g)
#> [1] 426
#Grab a three band combination to view as false color
f<-g[[c(52,88,117)]]
plotRGB(f,stretch="lin")
```
